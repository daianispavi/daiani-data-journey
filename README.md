# daiani-data-journey

# Sprint 1 - ETL com Python + PostgreSQL

Este projeto faz parte da minha jornada de atualiza√ß√£o para Engenharia de Dados Moderno.  
A Sprint 1 consiste em um pipeline simples que coleta dados meteorol√≥gicos da API [Open-Meteo](https://open-meteo.com/), transforma os dados com pandas e carrega no banco PostgreSQL local.

## Tecnologias utilizadas

- Python
- pandas
- requests
- PostgreSQL
- SQLAlchemy

### Configura√ß√µes sens√≠veis (.env)

Crie um arquivo **.env** na raiz do projeto, copiando o template **.env.example**, e preencha:

PG_URI=postgresql://pipeline_data:<SUA_SENHA>@localhost:5432/pipeline_db
LOG_LEVEL=INFO

> N√£o versione o '.env'. Ele j√° est√° no '.gitignore'.

## Como executar

### Criar banco e rodar pipeline

# 1. Instalar depend√™ncias
pip install -r requirements.txt

# 2. Criar usu√°rio e banco (uma vez)
psql -U postgres -c "CREATE DATABASE pipeline_db;"
psql -U postgres -c "CREATE USER pipeline_data WITH PASSWORD 'senha';"
psql -U postgres -d pipeline_db -c "GRANT ALL ON DATABASE pipeline_db TO pipeline_data;"

# 3. Preencher .env
PG_URI=postgresql://pipeline_data:senha@localhost:5432/pipeline_db

# 4. Executar
python pipeline.py

## Resultado

Print abaixo mostra os dados carregados na tabela 'temperatura_chapeco' via pgAdmin.

üì∑ *(incluir print aqui depois)*